{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T19:56:47.111528Z","iopub.status.idle":"2022-05-03T19:56:47.111822Z","shell.execute_reply.started":"2022-05-03T19:56:47.111663Z","shell.execute_reply":"2022-05-03T19:56:47.111682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions\nimport re, unicodedata, contractions, collections\nimport numpy as np \nimport pandas as pd \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nfrom nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom nltk import FreqDist\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:50.228900Z","iopub.execute_input":"2022-05-03T19:56:50.229146Z","iopub.status.idle":"2022-05-03T19:57:00.916131Z","shell.execute_reply.started":"2022-05-03T19:56:50.229118Z","shell.execute_reply":"2022-05-03T19:57:00.915359Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/natural-language-processing-with-disaster-tweets/kaggle nlp/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:57:07.862920Z","iopub.execute_input":"2022-05-03T19:57:07.863181Z","iopub.status.idle":"2022-05-03T19:57:07.908989Z","shell.execute_reply.started":"2022-05-03T19:57:07.863153Z","shell.execute_reply":"2022-05-03T19:57:07.908523Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Preliminary Analysis of Data","metadata":{}},{"cell_type":"code","source":"# Checking for null values \ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.115509Z","iopub.status.idle":"2022-05-03T19:56:47.115792Z","shell.execute_reply.started":"2022-05-03T19:56:47.115635Z","shell.execute_reply":"2022-05-03T19:56:47.115654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For figuring out how many null values are in which category \ntemp_df = pd.DataFrame(data = df['target'][df.keyword.isnull() == True].value_counts().values, columns = ['keyword'],index = df.target.value_counts().index)\ntemp_df['location'] = df['target'][df.location.isnull() == True].value_counts().values\ntemp_df.plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.116705Z","iopub.status.idle":"2022-05-03T19:56:47.117001Z","shell.execute_reply.started":"2022-05-03T19:56:47.116843Z","shell.execute_reply":"2022-05-03T19:56:47.116862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the non null values \ntext = df.loc[df.location.notnull(), 'location'].values \ntext = \" \".join(i for i in text) \nstopwords = set(STOPWORDS)\n\n# creating a wordcloud for visualization\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\nplt.figure( figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.117917Z","iopub.status.idle":"2022-05-03T19:56:47.118224Z","shell.execute_reply.started":"2022-05-03T19:56:47.118045Z","shell.execute_reply":"2022-05-03T19:56:47.118064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a word cloud from frequencies \n\n# creating a dictionary of keys and frequency count using value_counts\nvalues = df['keyword'].value_counts().keys().tolist()\ncounts = df['keyword'].value_counts().tolist()\nfreq = dict(zip(values, counts))\n\n# Feeding it to wordcloud\nwordcloud = wordcloud.generate_from_frequencies(freq)\nplt.figure( figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.119105Z","iopub.status.idle":"2022-05-03T19:56:47.119421Z","shell.execute_reply.started":"2022-05-03T19:56:47.119251Z","shell.execute_reply":"2022-05-03T19:56:47.119271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to figure out if a keyword actually exist in the text column\ndef match(x,y):\n    x = x.lower()\n    if pd.isnull(y):\n        y = 'aleeewooo'\n    else :\n        y = y.lower()\n    if x.find(y) != -1:\n        return 1\n    else:\n        return 0\n    \n# making a new match column in the df\ntemp = []\nfor i in range(df.shape[0]):\n    temp.append(match(df.text[i], df.keyword[i]))\n\ndf['match'] = temp\n\n# Cases where keyword exist but there is no match, replacing keywords with nan values \ndf['keyword'][(df.match == 0) & (df.keyword.notnull())] = np.nan\n\n# dropping match column \ndf.drop('match', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.121098Z","iopub.status.idle":"2022-05-03T19:56:47.121645Z","shell.execute_reply.started":"2022-05-03T19:56:47.121376Z","shell.execute_reply":"2022-05-03T19:56:47.121402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of each word\ndf['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n\n# unique words in each text \ndf['unique_words'] = df['text'].apply(lambda x : len(set(str(x).split())))\n\n# stopwords in each text \ndf['stopword_count'] = df['text'].apply(lambda x : len([word for word in str(x).split() if word in STOPWORDS]))\n\n# char len in each text \ndf['char_len'] = df['text'].apply(lambda x : len(str(x)))\n\n# this can also be done for punctuations, mentions etc. ","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.123170Z","iopub.status.idle":"2022-05-03T19:56:47.123631Z","shell.execute_reply.started":"2022-05-03T19:56:47.123397Z","shell.execute_reply":"2022-05-03T19:56:47.123421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['word_count','unique_words', 'stopword_count', 'char_len']","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.124487Z","iopub.status.idle":"2022-05-03T19:56:47.124880Z","shell.execute_reply.started":"2022-05-03T19:56:47.124665Z","shell.execute_reply":"2022-05-03T19:56:47.124687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=1, nrows=len(features), figsize=(8, 10), dpi=100)\n\nfor i, fea in enumerate(features):\n    sns.distplot(df[df.target == 1][fea], ax=axes[i], label = 'Disaster',color='red')\n    sns.distplot(df[df.target == 0][fea], ax=axes[i], label = 'Not Disaster',color='blue')\n    axes[i].legend()\n    axes[i].tick_params(axis='x', labelsize=8)\n    axes[i].tick_params(axis='y', labelsize=8)\n    \n    axes[i].set_title(f'{fea} Target Distribution in Training Set', fontsize=8)\n\nfig.tight_layout()\nplt.show()\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T19:56:47.126111Z","iopub.status.idle":"2022-05-03T19:56:47.126476Z","shell.execute_reply.started":"2022-05-03T19:56:47.126281Z","shell.execute_reply":"2022-05-03T19:56:47.126304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using a pretrained Glove word embedding for finding incorrect words","metadata":{}},{"cell_type":"code","source":"# used to delete output zip files\nimport os\nos.remove(\"/kaggle/working/glove.6B.zip\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:27:37.775453Z","iopub.execute_input":"2022-05-03T20:27:37.775946Z","iopub.status.idle":"2022-05-03T20:27:37.880796Z","shell.execute_reply.started":"2022-05-03T20:27:37.775914Z","shell.execute_reply":"2022-05-03T20:27:37.879823Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using a pretrained glove embedding\n\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip \"/kaggle/working/glove.6B.zip\" -d \"/content/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:27:56.901401Z","iopub.execute_input":"2022-05-03T20:27:56.901767Z","iopub.status.idle":"2022-05-03T20:30:58.317352Z","shell.execute_reply.started":"2022-05-03T20:27:56.901732Z","shell.execute_reply":"2022-05-03T20:30:58.316720Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# taking out the embedding dictionary for glove \n\nemmbed_dict = {}\nwith open('/content/glove.6B.200d.txt','r') as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:],'float32')\n    emmbed_dict[word]=vector\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:31:26.802897Z","iopub.execute_input":"2022-05-03T20:31:26.803165Z","iopub.status.idle":"2022-05-03T20:31:49.008978Z","shell.execute_reply.started":"2022-05-03T20:31:26.803138Z","shell.execute_reply":"2022-05-03T20:31:49.008120Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# making a list of all words that dont exist in the glove corpus.\nwrong_words = {}\nfor text in df.text:\n    for word in text.split():\n        if (word.lower() in wrong_words) and (word.lower() not in emmbed_dict.keys()):\n            wrong_words[word.lower()] += 1 \n        if (word.lower() not in wrong_words) and (word.lower() not in emmbed_dict.keys()):\n            wrong_words[word.lower()] = 1 \n\n# sorting by count             \nwrong_words = sorted(wrong_words.items(), key=lambda x: x[1], reverse = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:02.099738Z","iopub.execute_input":"2022-05-03T20:32:02.099972Z","iopub.status.idle":"2022-05-03T20:32:02.262153Z","shell.execute_reply.started":"2022-05-03T20:32:02.099948Z","shell.execute_reply":"2022-05-03T20:32:02.261428Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print('Total length of the words not in corpus of GLOVE pretrained model %d.'%(len(wrong_words)))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:05.115969Z","iopub.execute_input":"2022-05-03T20:32:05.117011Z","iopub.status.idle":"2022-05-03T20:32:05.121531Z","shell.execute_reply.started":"2022-05-03T20:32:05.116948Z","shell.execute_reply":"2022-05-03T20:32:05.120910Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# plotting the top 100 wrong words with their count\nx = list(dict(wrong_words).keys())[:100]\ny = list(dict(wrong_words).values())[:100]\n\nfig, axes = plt.subplots(figsize=(8, 20), dpi=100)\nsns.barplot(x = y,  y = x)\naxes.tick_params(axis = 'y', labelsize = 8)\n  ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T20:25:23.051740Z","iopub.execute_input":"2022-05-02T20:25:23.052018Z","iopub.status.idle":"2022-05-02T20:25:24.700539Z","shell.execute_reply.started":"2022-05-02T20:25:23.051988Z","shell.execute_reply":"2022-05-02T20:25:24.699868Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"code","source":"# turning all the text to smaller word -- Sómě becomes Some\ndef small_word(x):\n    text = ' '.join([word.lower() for word in x.split()])\n    return text\n\ndf.text = df.text.apply(small_word)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:10.703488Z","iopub.execute_input":"2022-05-03T20:32:10.704080Z","iopub.status.idle":"2022-05-03T20:32:10.750114Z","shell.execute_reply.started":"2022-05-03T20:32:10.704047Z","shell.execute_reply":"2022-05-03T20:32:10.749310Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Removing all texts with urls \ndef remove_URL(x):\n    text = ' '.join([re.sub(r'https?://\\S+|www\\.\\S+', \"\", word) for word in x.split()])\n    return text\n\ndf.text = df.text.apply(remove_URL)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:12.986802Z","iopub.execute_input":"2022-05-03T20:32:12.987788Z","iopub.status.idle":"2022-05-03T20:32:13.202014Z","shell.execute_reply.started":"2022-05-03T20:32:12.987734Z","shell.execute_reply":"2022-05-03T20:32:13.201472Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# replace special letters with normal letters \ndef remove_accented_chars(text):\n    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return new_text\n\ndf.text = df.text.apply(remove_accented_chars)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:15.740004Z","iopub.execute_input":"2022-05-03T20:32:15.740728Z","iopub.status.idle":"2022-05-03T20:32:15.762187Z","shell.execute_reply.started":"2022-05-03T20:32:15.740699Z","shell.execute_reply":"2022-05-03T20:32:15.761415Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# removing all special characters(non english letters also), @, emoji etc\ndef rem_ch(x):\n    x = ' '.join([re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", word) for word in x.split()])\n    return x \n\ndf.text = df.text.apply(rem_ch)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:18.409187Z","iopub.execute_input":"2022-05-03T20:32:18.409462Z","iopub.status.idle":"2022-05-03T20:32:18.812183Z","shell.execute_reply.started":"2022-05-03T20:32:18.409434Z","shell.execute_reply":"2022-05-03T20:32:18.811614Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"#function to tokenize \ndef tok(x):\n    x = word_tokenize(x)\n    return x \n\n#tokenizing the column\ndf['text'] = df['text'].apply(tok)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T14:41:49.803802Z","iopub.execute_input":"2022-04-20T14:41:49.804458Z","iopub.status.idle":"2022-04-20T14:41:50.720922Z","shell.execute_reply.started":"2022-04-20T14:41:49.804424Z","shell.execute_reply":"2022-04-20T14:41:50.719808Z"}}},{"cell_type":"code","source":"# Getting rid of the stopwords \ndef stopword(x):\n    text = ' '.join([word.lower() for word in x.split() if word.lower() not in (stopwords.words('english'))])\n    return text\n\n# Applying it to data \ndf.text = df.text.apply(stopword)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:24.945864Z","iopub.execute_input":"2022-05-03T20:32:24.946121Z","iopub.status.idle":"2022-05-03T20:32:41.907268Z","shell.execute_reply.started":"2022-05-03T20:32:24.946093Z","shell.execute_reply":"2022-05-03T20:32:41.906564Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"#### function to stem \ndef stem(x):\n    stemmer = LancasterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in x]\n    return stemmed_words\n\n#### applying it to data\ndf.text = df.text.apply(stem)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T11:20:44.906136Z","iopub.execute_input":"2022-04-16T11:20:44.906548Z","iopub.status.idle":"2022-04-16T11:20:47.98014Z","shell.execute_reply.started":"2022-04-16T11:20:44.906515Z","shell.execute_reply":"2022-04-16T11:20:47.97939Z"}}},{"cell_type":"code","source":"# function to lemmatize\ndef lemm(x):\n    lemmatizer = WordNetLemmatizer()\n    lemm_words = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n    return lemm_words\n\n# applying it to data\ndf.text = df.text.apply(lemm)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:41.909300Z","iopub.execute_input":"2022-05-03T20:32:41.909534Z","iopub.status.idle":"2022-05-03T20:32:44.954896Z","shell.execute_reply.started":"2022-05-03T20:32:41.909502Z","shell.execute_reply":"2022-05-03T20:32:44.953484Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Incorrect words analysis after text preprocessing","metadata":{}},{"cell_type":"code","source":"# making a list of all words that dont exist in the glove corpus.\nwrong_words_2 = {}\nfor text in df.text:\n    for word in text.split():\n        if (word in wrong_words_2) and (word not in emmbed_dict.keys()):\n            wrong_words_2[word] += 1 \n        if (word not in wrong_words_2) and (word not in emmbed_dict.keys()):\n            wrong_words_2[word] = 1 \n\n# sorting by count             \nwrong_words_2 = sorted(wrong_words_2.items(), key=lambda x: x[1], reverse = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:32:44.956040Z","iopub.execute_input":"2022-05-03T20:32:44.956277Z","iopub.status.idle":"2022-05-03T20:32:45.028017Z","shell.execute_reply.started":"2022-05-03T20:32:44.956246Z","shell.execute_reply":"2022-05-03T20:32:45.027245Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"print('Total length of the words not in corpus of GLOVE pretrained model after changes made ' + str(len(wrong_words_2)) +\n     '. Previously, it was ' + str(len(wrong_words)) + '.')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:37:17.045927Z","iopub.execute_input":"2022-05-03T20:37:17.046184Z","iopub.status.idle":"2022-05-03T20:37:17.050729Z","shell.execute_reply.started":"2022-05-03T20:37:17.046157Z","shell.execute_reply":"2022-05-03T20:37:17.049952Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Word Cloud for the text \nn_text = \" \".join(i for i in df.text) \n\n\nwordcloud = WordCloud(background_color=\"white\").generate(n_text)\nplt.figure( figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T10:18:12.701683Z","iopub.execute_input":"2022-04-24T10:18:12.702006Z","iopub.status.idle":"2022-04-24T10:18:14.035828Z","shell.execute_reply.started":"2022-04-24T10:18:12.701973Z","shell.execute_reply":"2022-04-24T10:18:14.034769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a dictonary with word frequency\nW = n_text.split()\nfdist = FreqDist(W)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T10:22:37.434582Z","iopub.execute_input":"2022-04-24T10:22:37.435009Z","iopub.status.idle":"2022-04-24T10:22:37.500917Z","shell.execute_reply.started":"2022-04-24T10:22:37.434978Z","shell.execute_reply":"2022-04-24T10:22:37.500258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tfidf vectorization \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nv = TfidfVectorizer()\nX = v.fit_transform(df['text'])\ny = df.target.values","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:39:11.529931Z","iopub.execute_input":"2022-05-03T20:39:11.530233Z","iopub.status.idle":"2022-05-03T20:39:11.716622Z","shell.execute_reply.started":"2022-05-03T20:39:11.530179Z","shell.execute_reply":"2022-05-03T20:39:11.716020Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X.toarray(),y, test_size=0.2, random_state= 42)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:39:13.920128Z","iopub.execute_input":"2022-05-03T20:39:13.920588Z","iopub.status.idle":"2022-05-03T20:39:14.307116Z","shell.execute_reply.started":"2022-05-03T20:39:13.920548Z","shell.execute_reply":"2022-05-03T20:39:14.306560Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# training on XGBoost \n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:39:17.775900Z","iopub.execute_input":"2022-05-03T20:39:17.776578Z","iopub.status.idle":"2022-05-03T20:41:32.515118Z","shell.execute_reply.started":"2022-05-03T20:39:17.776532Z","shell.execute_reply":"2022-05-03T20:41:32.514597Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:41:32.516556Z","iopub.execute_input":"2022-05-03T20:41:32.516917Z","iopub.status.idle":"2022-05-03T20:41:32.811070Z","shell.execute_reply.started":"2022-05-03T20:41:32.516886Z","shell.execute_reply":"2022-05-03T20:41:32.810598Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# This version leads to f1 score of 0.77\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T20:41:32.813970Z","iopub.execute_input":"2022-05-03T20:41:32.814672Z","iopub.status.idle":"2022-05-03T20:41:32.826773Z","shell.execute_reply.started":"2022-05-03T20:41:32.814644Z","shell.execute_reply":"2022-05-03T20:41:32.825750Z"},"trusted":true},"execution_count":77,"outputs":[]}]}